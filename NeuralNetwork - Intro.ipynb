{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "## Introduction\n",
    "\n",
    "An artificial Neural Network (NN) is a computational model to approximate complex, non-linear functions. Given a set of inputs $x$, and some internal parameters $\\omega$, the NN produces the corresponding outputs \n",
    "$y = f\\left(x; \\omega \\right)$. The NN is just a clever way of writing $f$, inspired by the structure of biological brains. The simplest form is the feed-forward NN sketched below:\n",
    "<table>\n",
    "<tr><td>\n",
    "<img src=\"images/nnscheme.png\" width=\"400px\">\n",
    "</td></tr>\n",
    "<tr><td>\n",
    "Schematic representation of a feed forward neural network.\n",
    "</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input values are fed to a layer of simple computational units, called <i>neurons</i>, which combine them with the parameters $\\omega$ to calculate their <i>activation</i> output. These values are then sent to the neurons in the output layer, which, in the same way, compute the final output $y$.\n",
    "\n",
    "Each $j$-th neuron in a layer computes its activation $a_j$ as:\n",
    "<table>\n",
    "<tr><td>\n",
    "<img src=\"images/nnneuron.png\" width=\"400px\">\n",
    "</td></tr>\n",
    "<tr><td>\n",
    "Neuron connection mechanism.\n",
    "</td></tr>\n",
    "</table>\n",
    "\n",
    "The parameters $\\omega_{j,i}$ are connection weights, while $b_j$ are internal neuron biases. All of these parameters make up the NN parameter set $\\omega$.\n",
    "The activation function $S$ is arbitrary, as long as it is non-linear. \n",
    "Most common activation functions are hyperbolic tangent and rectified linear unit (ReLU). \n",
    "In some cases, the activation function of output neurons is just linear (weighted sum of inputs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Here we use a NN to approximate an analytical function, called Model. In reality we wouldn't know the analytical expression for this function, just its values at some reference points. The knowledge can be exploited to train a neural network that mimics the original function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INITIAL DEFINITIONS ---\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy, math, random\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "# This function takes a 2-element vector p=[x,y] and computes z\n",
    "def Model(p):\n",
    "    result = math.sin(0.4*p[0] - 0.3*p[1] + 0.2*p[0]*p[1])\n",
    "    result += (2*random.random()-1)*0.1;\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the model function that will approximate:\n",
    "<img src=\"./images/nnsurface.png\">\n",
    "A small random noise is added to the function... to spice things up!\n",
    "\n",
    "Now we use the function to create a database of reference points, divided into <i>training</i> and <i>validation</i> sets.\n",
    "\n",
    "The training set provides the NN with a set of <i>examples</i> to learn from, while the validation set is later used to check the quality of the trained NN.\n",
    "The following code creates a list of $\\left(x,y\\right)$ points on the real plane, with $x,y \\in \\left[-1,1 \\right]$, to use as inputs.\n",
    "By applying the Model function to each one, we get a list of corresponding output:\n",
    "\n",
    "$z = \\sin\\left( 0.4x - 0.3y + 0.2xy \\right) + \\mathrm{noise}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training points\n",
    "nTrainPts = 100\n",
    "trainIn = 2*numpy.random.rand(nTrainPts,2) -1 # (x,y) points beween -1 and 1\n",
    "\n",
    "# validation points\n",
    "nValidPts = 40\n",
    "validIn = 2*numpy.random.rand(nValidPts,2)-1 # (x,y) points beween -1 and 1\n",
    " \n",
    "# apply Model to the points to compute z for the two sets\n",
    "trainOut = numpy.apply_along_axis(Model, 1, trainIn)\n",
    "validOut = numpy.apply_along_axis(Model, 1, validIn)\n",
    "\n",
    "# visualise the training set\n",
    "plt.scatter(trainIn[:,0], trainIn[:,1], c=trainOut[:])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a NN suitable for this problem. \n",
    "We can start with one hidden layer, containing 10 neurons. Initially all NN parameters are random.\n",
    "\n",
    "There are lots of initialisation parameters to chose from, and some may be important to train successfully.\n",
    "Check the scikit-learn <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\">documentation</a> for a list of parameters and their meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the neural network - 10 hidden neurons in 1 layer\n",
    "# the network uses tanh function on all hidden neurons\n",
    "# alpha is a regularisation parameter, explained later\n",
    "nn = MLPRegressor(hidden_layer_sizes=(10),  activation='tanh', solver='adam', alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now comes the tough part! \n",
    "The idea of training is to evaluate the NN on the training inputs and measure the error between its output and the reference values:\n",
    "\n",
    "$$ E = \\sum_{i}^{\\mathrm{samples}} \\left( y_i - y_i^{\\mathrm{ref}} \\right)^2$$\n",
    "\n",
    "It is then possible to compute the derivative (gradient) of the error w.r.t. each parameter (connections and biases):\n",
    "\n",
    "$$ \\mathbf{G} = \\frac{\\partial E}{\\partial \\mathbf{\\omega}} $$\n",
    "\n",
    "By shifting the parameters in the opposite direction of the gradient, we obtain a better set of parameters, that should give smaller error. This procedure can be repeated until $E$ is minimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some NN parameters can be changed here, without re-initialising the whole thing\n",
    "nn.set_params(learning_rate='adaptive', solver='lbfgs')\n",
    "nn.fit(trainIn, trainOut);\n",
    "\n",
    "# evaluate the training and validation error\n",
    "trainMLOut = nn.predict(trainIn)\n",
    "validMLOut = nn.predict(validIn)\n",
    "\n",
    "print (\"Mean Abs Error (training)  : \" + str (numpy.abs(trainMLOut-trainOut)).mean())\n",
    "print (\"Mean Abs Error (validation): \" + str (numpy.abs(validMLOut-validOut)).mean())\n",
    "\n",
    "# make a nice plot\n",
    "plt.plot(validOut,validMLOut,'o')\n",
    "plt.plot([-1,1],[-1,1]) # perfect fit line\n",
    "plt.xlabel('correct output')\n",
    "plt.ylabel('NN output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks: regularisation\n",
    "\n",
    "When we created the NN we set its regularisation property <i>alpha</i>, without explaining it: here is what it's about!\n",
    "\n",
    "In many cases, there are lots of neurons and thus parameters (complex model) but not a lot of data to train them. We might need such complex model for our purpose, but as a rule of thumb there should be at least 10 data points per parameter.\n",
    "\n",
    "If not, we are most likely over-fitting the data, i.e. we get a NN that brilliantly fits the training points, but utterly fails in validation. This is analogous to fitting a bunch of linearly correlated $\\left(x,y\\right)$ points with a 1000-degree polynomial. The fit might go through all the points, but it has no predictive power!\n",
    "\n",
    "Instead, we would like to train the NN to be as accurate <b>and</b> simple as possible, and one way to simplify a NN with lots of neurons is setting some useless connections to zero.\n",
    "\n",
    "The complexity of the network $R$, or regularisation factor, can be estimated e.g. by the sum of squared parameters:\n",
    "\n",
    "$$R = \\sum_{i} \\omega_i^2$$\n",
    "\n",
    "and the quantity to minimise during training becomes:\n",
    "\n",
    "$$ E + \\alpha R = \\sum_{i}^{\\mathrm{samples}} \\left( y_i - y_i^{\\mathrm{ref}} \\right)^2 + \\alpha \\sum_{i} \\omega_i^2$$\n",
    "\n",
    "The gradient of $R$ w.r.t. the parameters steers the training towards smaller weights at the expense of accuracy. One can chose how much the regularisation affects the training with the parameter <i>alpha</i>."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
